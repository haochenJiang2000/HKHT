encoder: 'bart'
bart: 'pretrained_models/bart-large-chinese'
n_embed: 1024
token_dropout: .2
dropout: .1
encoder_dropout: .0
decoder_dropout: .0
beam_size: 12
max_len: 1024
length_penalty: 1.
topk: 1
find_unused_parameters: 0
lr: 3e-05
lr_rate: 1
mu: .9
nu: .999
eps: 1e-8
weight_decay: 0.0
clip: 1.0
min_freq: 2
fix_len: 20
warmup_steps: 200
update_steps: 5
epochs: 60
patience: 5
batch_size: 16384
label_smoothing: 0.1
